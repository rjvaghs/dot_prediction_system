volumes:
  pgdata:
  minio-data:

services:
  postgres:
    image: postgres:15
    container_name: mlflow-postgres
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    volumes:
      - pgdata:/var/lib/postgresql/data
    ports:
      - "5432:5433"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}"]
      interval: 5s
      timeout: 3s
      retries: 10

  minio:
    image: minio/minio:latest
    container_name: mlflow-minio
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
    volumes:
      - minio-data:/data
    command: server /data --console-address ":9001"
    ports:
      - "9000:9000"
      - "9001:9001"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:${MINIO_PORT}/minio/health/live"]
      interval: 5s
      timeout: 3s
      retries: 20

  create-bucket:
    image: minio/mc:latest
    container_name: mlflow-create-bucket
    depends_on:
      minio:
        condition: service_healthy
    entrypoint:
      - /bin/sh
      - -c
      - |
        mc alias set myminio "http://${MINIO_HOST}:${MINIO_PORT}" \
          "${MINIO_ROOT_USER}" "${MINIO_ROOT_PASSWORD}"
        mc mb --ignore-existing "myminio/${MINIO_BUCKET:-mlflow}"
    restart: "no"

  mlflow:
    build: ./
    container_name: mlflow-server
    depends_on:
      postgres:
        condition: service_healthy
      minio:
        condition: service_healthy
      create-bucket:
        condition: service_completed_successfully
    environment:
      # Backend store URI built from vars
      MLFLOW_BACKEND_STORE_URI: ${MLFLOW_BACKEND_STORE_URI}

      # S3/MinIO settings
      MLFLOW_S3_ENDPOINT_URL: ${MLFLOW_S3_ENDPOINT_URL}
      MLFLOW_ARTIFACTS_DESTINATION: ${MLFLOW_ARTIFACTS_DESTINATION}
      AWS_ACCESS_KEY_ID: ${MINIO_ROOT_USER}
      AWS_SECRET_ACCESS_KEY: ${MINIO_ROOT_PASSWORD}
      AWS_DEFAULT_REGION: ${AWS_DEFAULT_REGION}
      MLFLOW_S3_IGNORE_TLS: "true"
      MLFLOW_TRACKING_URI: http://localhost:5000

      # Server host/port
      MLFLOW_HOST: ${MLFLOW_HOST}
      MLFLOW_PORT: ${MLFLOW_PORT}

    command:
      - /bin/bash
      - -c
      - |
        pip install --no-cache-dir psycopg2-binary boto3
        mlflow server \
          --backend-store-uri "${MLFLOW_BACKEND_STORE_URI}" \
          --artifacts-destination "${MLFLOW_ARTIFACTS_DESTINATION}" \
          --serve-artifacts \
          --host "${MLFLOW_HOST}" \
          --port "${MLFLOW_PORT}" \
          --allowed-hosts "*"
    ports:
      - "${MLFLOW_PORT}:${MLFLOW_PORT}"
    healthcheck:
      test:
        [
          "CMD",
          "python",
          "-c",
          "import urllib.request; urllib.request.urlopen('http://localhost:${MLFLOW_PORT}/health')",
        ]
      interval: 10s
      timeout: 5s
      retries: 30

  # ---------------------------
  # Training Service
  # ---------------------------
  training:
    build: ./training
    container_name: dot_training
    depends_on:
      - mlflow
    environment:
      MLFLOW_TRACKING_URI: http://mlflow:5000
    volumes:
      - ./data:/app/data
    restart: "no"

  # ---------------------------
  # Model Serving API
  # ---------------------------
  # model_serving_518:
  #   build: ./
  #   container_name: dot_model_serving_518
  #   depends_on:
  #     - mlflow
  #   entrypoint: >
  #     mlflow models serve
  #     -m models:/station_1000000518/latest
  #     -p 8000
  #     --host 0.0.0.0
  #     --env-manager "local"
  #   environment:
  #     MLFLOW_TRACKING_URI: http://mlflow:5000
  #   ports:
  #     - "8000:8000"

  # ---------------------------
  # Model Serving API
  # ---------------------------
  model_serving_471:
    build: ./
    container_name: dot_model_serving_471
    depends_on:
      - mlflow
    entrypoint: >
      mlflow models serve
      -m models:/station_1000000471/latest
      -p 7001
      --host 0.0.0.0
      --env-manager "local"
    environment:
      MLFLOW_TRACKING_URI: http://mlflow:5000
    ports:
      - "7001:7001"

     # ---------------------------
  # Model Serving API
  # ---------------------------
  model_serving_523:
    build: ./
    container_name: dot_model_serving_523
    depends_on:
      - mlflow
    entrypoint: >
      mlflow models serve
      -m models:/station_1000000523/latest
      -p 7002
      --host 0.0.0.0
      --env-manager "local"
    environment:
      MLFLOW_TRACKING_URI: http://mlflow:5000
    ports:
      - "7002:7002"

networks:
  default:
    name: mlflow-network

